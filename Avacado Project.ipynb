{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avacado Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <b> Problem Statement:</b>\n",
    "<em><b>\n",
    "Avocado is a fruit consumed by people heavily in the United States. \n",
    "</b></em>\n",
    "\n",
    "Content : \n",
    "    \n",
    "This data was downloaded from the Hass Avocado Board website in May of 2018 & compiled into a single CSV. \n",
    "\n",
    "The table below represents weekly 2018 retail scan data for National retail volume (units) and price. Retail scan data comes directly from retailers’ cash registers based on actual retail sales of Hass avocados. \n",
    "     \n",
    "Starting in 2013, the table below reflects an expanded, multi-outlet retail data set. Multi-outlet reporting includes an aggregation of the following channels: grocery, mass, club, drug, dollar and military. The Average Price (of avocados) in the table reflects a per unit (per avocado) cost, even when multiple units (avocados) are sold in bags. The Product Lookup codes (PLU’s) in the table are only for Hass avocados. Other varieties of avocados (e.g. greenskins) are not included in this table.\n",
    "\n",
    "- <b>Some relevant columns in the dataset:\n",
    "\n",
    "    - Date - The date of the observation\n",
    "    - AveragePrice - the average price of a single avocado\n",
    "    - type - conventional or organic\n",
    "    - year - the year\n",
    "    - Region - the city or region of the observation\n",
    "    - Total Volume - Total number of avocados sold\n",
    "    - 4046 - Total number of avocados with PLU 4046 sold\n",
    "    - 4225 - Total number of avocados with PLU 4225 sold\n",
    "    - 4770 - Total number of avocados with PLU 4770 sold </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Inspiration /Label </b>\n",
    "\n",
    "The dataset can be seen in two angles to find the region and find the average price .\n",
    "\n",
    "Task: One of Classification and other of Regression\n",
    "\n",
    "Do both tasks in the same .ipynb file and submit at single file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>To download the dataset, use the link given below. \n",
    "\n",
    "Downlaod Files:</b>\n",
    "https://github.com/dsrscientist/Data-Science-ML-Capstone-Projects/blob/master/avocado.csv.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b> Importing require library for performing EDA, Data Wrangling and data cleaning</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # for data wrangling purpose\n",
    "import numpy as np # Basic computation library\n",
    "import seaborn as sns # For Visualization \n",
    "import matplotlib.pyplot as plt # ploting package\n",
    "%matplotlib inline\n",
    "import warnings # Filtering warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Avocado Csv file using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('avocado.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('No of Rows:',df.shape[0])\n",
    "print('No of Columns:',df.shape[1])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Unnamed: 0'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <b>  Before Going for Statistical exploration of data, first check integrity of data & Missing value </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Integrity Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Since dataset is large,  Let check for any entry which is repeated or duplicated in dataset at same date. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()  # This will check the duplicate data for all columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment:\n",
    "Dataset doesnot contain Any duplicate entry. So Yes To Go !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing value check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(df.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = df.isnull().sum().sort_values(ascending = False)\n",
    "percentage_missing_values =(missing_values/len(df))*100\n",
    "print(pd.concat([missing_values, percentage_missing_values], axis =1, keys =['Missing Values', '% Missing data']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment:\n",
    "- There is no missing values in dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datatype Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment :\n",
    "- There are 13 features in data.\n",
    "- Region has object datatype.\n",
    "- Date by default data type is object which need to change to datetime datatype.\n",
    "- Other variable are float & int datatypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting datatype of date column\n",
    "df['Date']=pd.to_datetime(df.Date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the statistics of the columns using heatmap.\n",
    "plt.figure(figsize=(20,8))\n",
    "sns.heatmap(df.describe(),linewidths = 0.1,fmt='0.1f',annot = True,cmap='PiYG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment :\n",
    "- We can easily suspect that some of the features contain outliers after looking at 50% and max columns.\n",
    "- For most of columns have minimum value of zero.\n",
    "- Almost in all columns value of mean is greater than median. Data is right skewed.\n",
    "- For most of columns value of std. deviation is greater than mean i.e. data is spread. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b> Lets do some Statistical Analysis. Start with find Average price of avocodo over the timeframe and according types </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\033[1m\"+'Minimum Price of Avocado :'+\"\\033[0m\",df.AveragePrice.min(),'USD')\n",
    "print(\"\\033[1m\"+'Maximum Price of avacado :'+\"\\033[0m\",df.AveragePrice.max(),'USD')\n",
    "print(\"\\033[1m\"+'Average Price of avacado :'+\"\\033[0m\",df.AveragePrice.mean(),'USD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avacado varies from 0.44 USD To 3.25 USD with average price of 1.40 USD per unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,7))\n",
    "plt.title('Distribution Price')\n",
    "sns.distplot(df[\"AveragePrice\"], color='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Most of cases Average price of Avacado varies between 1.1 and 1.6 USD/unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('type')['AveragePrice'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\033[1m\"+'Percentage difference in price of avacado :'+\"\\033[0m\",((1.65399-1.1580)/1.1580)*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.boxplot(y=\"type\", x=\"AveragePrice\", data=df, palette = 'hsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment :\n",
    "- Oraganic Avocado is almost <u> 43% more costlier </u> than conventional Avacado.\n",
    "- We can see that Average Price of Organic Avocado is higher than Conventional Avocado.\n",
    "- This obvious as cultivation of organic avocado is expensive, time taking process demanding much effort & patience.Also health cautious and quality oriented people are willing to pay more price for it.\n",
    "    \n",
    "<b> Before going deeper with Average price to gain more insight as it has lot more to explore, just let do quick check over volumetric sell of Avacado. Normal tendency of most people is they dont want to pay more price so most probably if such scenerio exist then it will be reflected in total sales.</b>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = 'Conventional','Organic',\n",
    "fig, ax = plt.subplots()\n",
    "ax.pie(df.groupby('type')['Total Volume'].mean(),labels = labels,radius =2,autopct = '%2.2f%%',explode=[0.3,0.2], shadow=True,)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('type')['Total Volume'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x='year', y='Total Volume', hue='type',data=df, palette='coolwarm',ci=68)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment -\n",
    "- The sell of organic avacodo is <u> less than 3 %.</u>\n",
    "- <b> Conventional Avacado are in more demand.</b>\n",
    "- Barplot confirm same thing in yearwise sales volume.\n",
    "\n",
    "<b> The Average price varies over year and season , lets dive to get some more insight over it.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.crosstab([df.type,df.year],df.AveragePrice, margins= True).style.background_gradient(cmap='summer_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['year','type'])['AveragePrice'].agg([min,max,np.mean,np.median])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment :\n",
    "- Oraganic Avocado is 40-50% costlier than conventional avacado.\n",
    "- Avacodo got price hike in year 2017 as result of some reason may be inflation,shortage, more wages.\n",
    "- Except 2017 each year their is incresase in price of avacado. \n",
    "    \n",
    "<b> Price hike over time, its okay !!!\n",
    "    \n",
    "But what about demand over timeframe ?, Lets check trend of demand through volume sell in next section. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['year','type'])['Total Volume','Total Bags','Small Bags','Large Bags','XLarge Bags'].agg([sum])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment :\n",
    "- Sales of Avacado in 2017 is more than 2016 so price hike is may be as result of increase in demand or some supply chain issue.\n",
    "- sales in turn Demand is also increase over time.\n",
    "- 30 % increase in sale of organic avocado in 2017 compare to 2016, even when it is almost 50% costlier than conventional type.\n",
    "- There is also growth in demand of conventional type avocado year-wise but rate of growth of demand comparelative less than organic type. May be people becoming more health oriented.\n",
    "- XLarge Bags sales increase rapidly in 2017 compare to 2015.\n",
    "- If we look at trend from 2015 customer start buying buying Large or Xlarge bags compare to small bags in case of Conventional type. May be quantity & discount offer by shop may be playing role here.\n",
    "- People choosing small bags of organic avocado over large.Price factor playing here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=['Date'], inplace=True, ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average price of Conventional Avocados over time\n",
    "\n",
    "mask = df['type']== 'conventional'\n",
    "plt.rc('figure', titlesize=20)\n",
    "fig = plt.figure(figsize = (27, 12))\n",
    "fig.suptitle('Average Price of Conventional Avocados Over Time', fontsize=25)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "dates = df[mask]['Date'].tolist()\n",
    "avgPrices = df[mask]['AveragePrice'].tolist()\n",
    "\n",
    "plt.scatter( dates,avgPrices, c=avgPrices, cmap='plasma')\n",
    "ax.set_xlabel('Date',fontsize = 15,)\n",
    "ax.set_ylabel('Average Price (USD)', fontsize = 15)\n",
    "ax.set_xlim()\n",
    "ax.tick_params(labelrotation=90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average price of Organic Avocados over time\n",
    "mask = df['type']== 'organic'\n",
    "plt.rc('figure', titlesize=20)\n",
    "fig = plt.figure(figsize = (27, 12))\n",
    "fig.suptitle('Average Price of Organic Avocados Over Time', fontsize=25)\n",
    "ax = fig.add_subplot(111)\n",
    "fig.subplots_adjust(top=0.93)\n",
    "\n",
    "dates = df[mask]['Date'].tolist()\n",
    "avgPrices = df[mask]['AveragePrice'].tolist()\n",
    "\n",
    "plt.scatter(dates,avgPrices, c=avgPrices, cmap='plasma')\n",
    "ax.set_xlabel('Date',fontsize = 15)\n",
    "ax.set_ylabel('Average Price (USD)', fontsize = 15)\n",
    "plt.xlim()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['region'])['Total Volume','AveragePrice'].agg([sum])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Datframe for numeric features\n",
    "df2.drop(['Date','type','region','year'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,25), facecolor='white')\n",
    "plotnumber =1\n",
    "for column in df2:\n",
    "    if plotnumber <=9:\n",
    "        ax = plt.subplot(3,3,plotnumber)\n",
    "        sns.distplot(df2[column], color='r',hist=False,kde_kws={\"shade\": True})\n",
    "        plt.xlabel(column,fontsize=20)\n",
    "    plotnumber+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Comment :\n",
    "- We can see skewed data with presence of outliers with sharp peak.\n",
    "- Most of the plots are right skewed, having value of mean very small compare to std. deviation & median. This is resulting in sharp high peak.Very few data points are present as we move away from zero to other end of x-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_palette('gist_rainbow_r')\n",
    "plt.figure(figsize=(20,20), facecolor='white')\n",
    "plotnumber =1\n",
    "for column in df2:\n",
    "    if plotnumber <=9:\n",
    "        ax = plt.subplot(3,3,plotnumber)\n",
    "        sns.violinplot(df[column])\n",
    "        plt.xlabel(column,fontsize=20)\n",
    "    plotnumber+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment :\n",
    "- For Majority Average price is in range of 1 to 1.5.\n",
    "- Rest of Violin plot show same story as distribution plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><em> Avacado availability and price also vary over the year according to seasonal changes.Lets check the variation Avg price and sales volume over each month.</em></b>\n",
    "    \n",
    "    For that purpose let create another copy dataframe and then split date column into Year,Month and Day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('avocado.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy of original dataframe\n",
    "df3=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['Year'], df3['Month'], df3['Day'] = df3['Date'].str.split('-').str\n",
    "df3.drop(columns=['Date','year'], inplace=True)\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting Datetime datatypes to int\n",
    "df3['Year']=df3['Year'].astype(int)\n",
    "df3['Month']=df3['Month'].astype(int)\n",
    "df3['Date']=df3['Day'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.drop(columns=['Day'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.drop(columns=['Date'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate analysis of feature using date, months, year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking unique values and Yearwise total No of sale entry\n",
    "df3['Year'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot('Year',data=df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Comment:\n",
    "    - This dataset contain data from 2015 to 2018\n",
    "    - Maximum sales entry belongs to year 2017 while very few sales entry comes from 2018. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach :\n",
    "- We want check Avg Price over each Month\n",
    "- First we will check Sale entries per Months\n",
    "- Next We will Avg Price over each month using Countplot\n",
    "- Finally we will try to bring insight over relation of sales entries to Avg price over same month. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking Month-Wise sales entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "sns.countplot(x='Month', data=df3, palette=\"spring\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['Month'].value_counts().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['Month'].value_counts().min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment:\n",
    "- Maximum sales lead comes in month of Jan, Feb, March\n",
    "- Maximum sales demand of Avacodo is in month of Jan (1944 lead) & Minimum sales demand of Avacoda is in month of June"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking Month-Wise Variation in Average Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "sns.barplot(x=\"Month\", y=\"AveragePrice\", hue='type', data=df3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.groupby(['Month','type'])['AveragePrice'].agg([np.mean]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "sns.catplot(x = \"Month\", y = \"AveragePrice\", kind ='violin', data=df3, linewidth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment :\n",
    "- In month of September and October Price of Conventional type of Avacado is comparatively higher than other months.\n",
    "- There is not much upheaval in Organic type Avacoda over year peroid.\n",
    "- We can see that variation in mean price values in above groupby table, which confirm what we got from barplot.\n",
    "    \n",
    "<b> But what causes such High demand in 1st quater of year ?\n",
    "\n",
    "Simple Google gives answer 'FLAVOR'. Yes, Flavor!!!\n",
    "    \n",
    "Avocados are available year round like most agricultural commodities these days, but <u> <em> January through March  is the best time of year for flavor</em> </u>. It is during this time that the fruit has developed higher oil content, resulting in that buttery flavor and texture that we all love.\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b> Now Let Compare Countplot of Sales Entries and Barplot of Avg Price Monthwise.</b> \n",
    "- In first sight we can see in period of Augest to October price high & in that same time period sales order are minimum.\n",
    "- Opposite of it true in duration of Jan to March.\n",
    "- This variation applicable to conventional type while we cannot find such any relation in case of Organic avacado.\n",
    "    \n",
    "<b>In conclusion when Average sale price of 'Conventional Avacado' is high in same time less sales order are observe. Inshort Customer are less interested in buying avacado at high Price. \n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Till Now we have not explore region feature , Now is time do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "sns.barplot(x=df['region'],y=df['AveragePrice'], data=df3)\n",
    "plt.title(f'Bar Plot for regions')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = df.groupby('region')['Total Volume'].sum().sort_values(ascending=False).reset_index()\n",
    "plt.figure(figsize=(15,13))\n",
    "sns.barplot(x=region[\"Total Volume\"], y=region[\"region\"], data=df, palette=\"Set1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['region'])['Total Volume','Total Bags','Small Bags','Large Bags','XLarge Bags'].agg([np.mean])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment on feature Region:\n",
    "- Maximum sales Volume is in South Central While Minimum Sales Volume is in Syracuse.\n",
    "- Maximum Average Price is in SanFrancisco and Minimum Average Sale price is in Houston.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,25),facecolor='white')\n",
    "plotnumber=1\n",
    "y = df[['4046','4225','4770']]\n",
    "X = df['year']\n",
    "for col in y:\n",
    "    if plotnumber<=9:\n",
    "        plt.subplot(3,3,plotnumber)\n",
    "        sns.barplot(X,y[col])\n",
    "        plt.xlabel('Year',fontsize=20)\n",
    "        plt.ylabel(col,fontsize=20)\n",
    "    plotnumber+=1\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Production of PLU 4046 has grown over the 4 years and is greater than the production of PLU 4225 and  PLU 4770, which is least produced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,25),facecolor='white')\n",
    "plotnumber=1\n",
    "y = df[['Small Bags','Large Bags','XLarge Bags']]\n",
    "X = df['year']\n",
    "for col in y:\n",
    "    if plotnumber<=9:\n",
    "        plt.subplot(3,3,plotnumber)\n",
    "        sns.barplot(X,y[col])\n",
    "        plt.xlabel('Year',fontsize=20)\n",
    "        plt.ylabel(col,fontsize=20)\n",
    "    plotnumber+=1\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment:\n",
    "- Sale of all 3 types of bags has grown over the years. \n",
    "- Most of customers prefer buying Small Bags over Large and Xlarge bags.\n",
    "- There is very narrow Customer segment willing to buy XLarge type bags possiblily more quantity need to consume and price associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df['type']=='conventional'\n",
    "g = sns.factorplot('AveragePrice','region',data=df[mask],\n",
    "                   hue='year',\n",
    "                   size=13,\n",
    "                   aspect=0.8,\n",
    "                   palette='magma',\n",
    "                   join=False,\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df['type']=='organic'\n",
    "g = sns.factorplot('AveragePrice','region',data=df[mask],\n",
    "                   hue='year',\n",
    "                   size=13,\n",
    "                   aspect=0.8,\n",
    "                   palette='magma',\n",
    "                   join=False,\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Average price of the avacado is high in the region San Francisco followed by RaleighGreensboro in the year 2017. The demand of the fruit is high and price also high in the year 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pairplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Label Encoder on target variable\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df3['region']=le.fit_transform(df3['region'])\n",
    "df3['type']=le.fit_transform(df3['type'])\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers Detection and Removal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,15), facecolor='white')\n",
    "plotnumber =1\n",
    "for column in df2:\n",
    "    if plotnumber <=9:\n",
    "        ax = plt.subplot(3,3,plotnumber)\n",
    "        sns.boxplot(df2[column], palette='hsv')\n",
    "        plt.xlabel(column,fontsize=20)\n",
    "    plotnumber+=1\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "z = np.abs(zscore(df3))\n",
    "threshold = 3\n",
    "df4 = df3[(z<3).all(axis = 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\033[1m\"+'Shape of dataset after removing outliers :'+\"\\033[0m\",df4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\033[1m\"+'Percentage Data Loss :'+\"\\033[0m\",((18249-17651)/18249)*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Corrleation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(25,18))\n",
    "sns.heatmap(df4.corr(), vmin=-1, vmax=1, annot=True, square=True, fmt='0.3f', \n",
    "            annot_kws={'size':10}, cmap=\"gist_stern\")\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (18,6))\n",
    "df4.corr()['AveragePrice'].drop(['AveragePrice']).plot(kind='bar',color = 'c')\n",
    "plt.xlabel('Features',fontsize=15)\n",
    "plt.ylabel('AveragePrice',fontsize=15)\n",
    "plt.title('Correlation of features with Target Variable Average Price',fontsize = 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Almost all features are highly negatively correlated with the target variable and only type,year,month have positive correlation with the target.</b>\n",
    "\n",
    "   - Small bags are highly correlated with Total Bags correlation coeffient 0.978. It is natural as most of bags are small.\n",
    "   - Total volume is highly correleated 4046."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment:\n",
    "<b> We are going to remove some of columns which are not reqire for further analysis</b>\n",
    "- 4046,4225,4770 are shows multicollinearity with Total Volume. These feature are nothing but Total avacado sold under particular grade, which are already counted in Total Volume. As we do not have any price data according each grade of avacado. These feature doesnot have any meaning in ML model.So gone drop them.\n",
    "- Same goes with Total bags. Total bags is sum of all other type of bags. We gone keep drop Total bags and going to keep other differnt size bags counts features.\n",
    "- Next is region feature, we also going drop this as it very poorly correlated with other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.drop(columns=['4046','4225','4770','region','Total Bags'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Checking Multicollinearity between features using variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "vif= pd.DataFrame()\n",
    "vif['VIF']= [variance_inflation_factor(df4.values,i) for i in range(df4.shape[1])]\n",
    "vif['Features']= df4.columns\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strategy to Address Multicollinearity :\n",
    "1. Removing Some of highly correlated features. But this will not work here as most of input features are correlated with each other either moderated or poorly.\n",
    "2. Another way to address Multicollinerity is to Scaled Data and then apply PCA.\n",
    "\n",
    "<b> We will go by first way for further investigation. As For Independent feature VIF is within limit of 10 except year.  </b>\n",
    "\n",
    "We will drop Year as it is less than 20% correlated with target feature and For most of input features correlated with less than 10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.drop(columns=['Year'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking correlation with target variable after removal of multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (14,5))\n",
    "df4.corr()['AveragePrice'].drop(['AveragePrice']).plot(kind='bar',color = 'c')\n",
    "plt.xlabel('Features',fontsize=15)\n",
    "plt.ylabel('AveragePrice',fontsize=15)\n",
    "plt.title('Correlation of features with Target Variable Average Price',fontsize = 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skewness of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df4.skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming skewness through log transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df4.columns:\n",
    "    if df4.skew().loc[col]>0.55:\n",
    "        df4[col]=np.log1p(df4[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df4.drop('AveragePrice', axis=1)\n",
    "Y = df4['AveragePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler= StandardScaler()\n",
    "X_scale = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import  GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import  Ridge\n",
    "from sklearn.linear_model import  Lasso\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scale, Y, random_state=42, test_size=.33)\n",
    "print('Training feature matrix size:',X_train.shape)\n",
    "print('Training target vector size:',Y_train.shape)\n",
    "print('Test feature matrix size:',X_test.shape)\n",
    "print('Test target vector size:',Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "maxR2_score=0\n",
    "maxRS=0\n",
    "for i in range(1,1000):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X_scale, Y, random_state=i, test_size=.33)\n",
    "    lin_reg=LinearRegression()\n",
    "    lin_reg.fit(X_train,Y_train)\n",
    "    y_pred=lin_reg.predict(X_test)\n",
    "    R2=r2_score(Y_test,y_pred)\n",
    "    if R2>maxR2_score:\n",
    "        maxR2_score=R2\n",
    "        maxRS=i\n",
    "print('Best R2 Score is', maxR2_score ,'on Random_state', maxRS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression : Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scale, Y, random_state=557, test_size=.33)\n",
    "lin_reg=LinearRegression()\n",
    "lin_reg.fit(X_train,Y_train)\n",
    "lin_reg.score(X_train,Y_train)\n",
    "y_pred=lin_reg.predict(X_test)\n",
    "print('\\033[1m'+'Predicted Wins:'+'\\033[0m\\n',y_pred)\n",
    "print('\\n')\n",
    "print('\\033[1m'+'Actual Wins:'+'\\033[0m\\n',Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Evaluation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "print('\\033[1m'+' Error :'+'\\033[0m')\n",
    "print('Mean absolute error :', mean_absolute_error(Y_test,y_pred))\n",
    "print('Mean squared error :', mean_squared_error(Y_test,y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(mean_squared_error(Y_test,y_pred)))\n",
    "print('\\n')\n",
    "from sklearn.metrics import r2_score\n",
    "print('\\033[1m'+' R2 Score :'+'\\033[0m')\n",
    "print(r2_score(Y_test,y_pred,multioutput='variance_weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "score = cross_val_score(lin_reg, X_scale, Y, cv =5)\n",
    "print('\\033[1m'+'Cross Validation Score :',lin_reg,\":\"+'\\033[0m\\n')\n",
    "print(\"Mean CV Score :\",score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying other Regression Model, Evaluation & Crossvalidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators = 70 ,max_depth=25)\n",
    "dtc = DecisionTreeRegressor()\n",
    "adb=AdaBoostRegressor(learning_rate=0.1)\n",
    "gradb=GradientBoostingRegressor(max_depth=25,learning_rate=0.1)\n",
    "rd=Ridge(alpha=0.01)\n",
    "xgb=XGBRegressor()\n",
    "model = [rf,rd,dtc,adb,gradb,xgb]\n",
    "\n",
    "for m in model:\n",
    "    m.fit(X_train,Y_train)\n",
    "    m.score(X_train,Y_train)\n",
    "    y_pred = m.predict(X_test)\n",
    "    print('\\n')                                        \n",
    "    print('\\033[1m'+' Error of ', m, ':' +'\\033[0m')\n",
    "    print('Mean absolute error :', mean_absolute_error(Y_test,y_pred))\n",
    "    print('Mean squared error :', mean_squared_error(Y_test,y_pred))\n",
    "    print('Root Mean Squared Error:', np.sqrt(mean_squared_error(Y_test,y_pred)))\n",
    "    print('\\n')\n",
    "\n",
    "    print('\\033[1m'+' R2 Score :'+'\\033[0m')\n",
    "    print(r2_score(Y_test,y_pred)) \n",
    "    print('==============================================================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "model = [rf,rd,dtc,adb,gradb,xgb]\n",
    "\n",
    "for m in model:\n",
    "    score = cross_val_score(m, X_scale, Y, cv =5)\n",
    "    print('\\n')\n",
    "    print('\\033[1m'+'Cross Validation Score :',m,\":\"+'\\033[0m\\n')\n",
    "    print(\"Mean CV Score :\",score.mean())\n",
    "    print('==============================================================================================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Among all Model Random Forest Regressor gave us maximum R2 score of 0.7891 and minimum       RMSE value  of 0.17930. So We will perform Hyper Parameter Tuning on Random Forest Regressor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameter Tuning : GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter = {'n_estimators':[30,60,80],'max_depth': [10,20,40],\n",
    "             'min_samples_leaf':[5,10],'criterion':['mse','mae'],\n",
    "             'max_features':[\"auto\",\"sqrt\",\"log2\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCV = GridSearchCV(RandomForestRegressor(),parameter,cv=5,n_jobs = -1,verbose = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "GCV.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> I waited for almost hour for 2-3 times, but I did not get any output of Grid search CV. This May be due to computational Limitation. So I am going forward with Final Model by Manual defining parameter.</b>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "GCV.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "Final_mod =  RandomForestRegressor(n_estimators=60 ,criterion = 'mse', max_depth= 20, max_features = 'auto',\n",
    "             min_samples_leaf = 5, min_samples_split = 10)\n",
    "Final_mod.fit(X_train,Y_train)\n",
    "y_pred=Final_mod.predict(X_test)\n",
    "print('\\n')                                        \n",
    "print('\\033[1m'+' Error in Final Model :' +'\\033[0m')\n",
    "print('Mean absolute error :', mean_absolute_error(Y_test,y_pred))\n",
    "print('Mean squared error :', mean_squared_error(Y_test,y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(mean_squared_error(Y_test,y_pred)))\n",
    "print('\\n')\n",
    "print('\\033[1m'+' R2 Score of Final Model :'+'\\033[0m')\n",
    "print(r2_score(Y_test,y_pred)) \n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,7))\n",
    "y_pred=Final_mod.predict(X_test)\n",
    "sns.swarmplot(Y_test.round(2), y_pred)\n",
    "print('\\033[1m'+' True Values Vs Predicted Value plot :' +'\\033[0m')\n",
    "plt.xlabel('True Values' , fontsize=15)\n",
    "plt.ylabel('Predictions', fontsize=15)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(Final_mod,'Avacado_Final.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
